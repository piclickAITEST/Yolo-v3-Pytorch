{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sys import platform\n",
    "\n",
    "from models import *  # set ONNX_EXPORT in models.py\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "\n",
    "cfg = 'cfg/fashion.cfg'\n",
    "data = 'data/fashion/fashion.data'\n",
    "weights = 'weights/exp1/best.pt'\n",
    "\n",
    "source = 'off_images/' \n",
    "#source = 'on_images/'\n",
    "\n",
    "output = 'output_off'\n",
    "\n",
    "img_size = 416\n",
    "conf_thres = 0.5\n",
    "nms_thres = 0.3\n",
    "half = False\n",
    "view_img = False\n",
    "device=''\n",
    "\n",
    "img_size = (320, 192) if ONNX_EXPORT else img_size\n",
    "out, source, weights, half, view_img = output, source, weights, half, view_img\n",
    "webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n",
    "# Initialize\n",
    "device = torch_utils.select_device(device='cpu' if ONNX_EXPORT else device)\n",
    "\n",
    "model = Darknet(cfg, img_size)\n",
    "\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    _ = load_darknet_weights(model, weights)\n",
    "\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Dataloader\n",
    "vid_path, vid_writer = None, None\n",
    "save_txt = True\n",
    "save_img = True\n",
    "dataset = LoadImages(source, img_size=img_size, half=half)\n",
    "\n",
    "# Get classes and colors\n",
    "classes = load_classes(parse_data_cfg(data)['names'])\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(classes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(out):\n",
    "    shutil.rmtree(out)  # delete output folder\n",
    "os.makedirs(out)  # make new output folder\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "no_list =[]\n",
    "yes_list = []\n",
    "\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    t = time.time()\n",
    "    \n",
    "    any_box = True\n",
    "    \n",
    "    \n",
    "    # Get detections\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred, _ = model(img)\n",
    "\n",
    "    for i, det in enumerate(non_max_suppression(pred, conf_thres, nms_thres)):  # detections per image\n",
    "        p, s, im0 = path, '', im0s\n",
    "        img_width, img_height = img.shape[2:]\n",
    "        \n",
    "        save_path = str(Path(out) / Path(p).name.split('.')[0])\n",
    "        if det is not None and len(det):\n",
    "            boxes = len(det)\n",
    "            \n",
    "            for *xyxy, conf, _, cls in det:\n",
    "\n",
    "                with open(save_path + '.txt', 'a') as file:\n",
    "                    file.write(('%g ' * 7 + '\\n') % (*xyxy, cls, img_width,img_height))\n",
    "                \n",
    "                if cls in (4,6,7,8): # 4 bag, 6 Headwear, 7 Acc, 8 Innerwear # 1차 제거\n",
    "                    boxes -= 1\n",
    "                    continue     \n",
    "                    \n",
    "                else: # 2차 제거 (박스 크기)\n",
    "                    \n",
    "                    x1,y1,x3,y3 = xyxy\n",
    "                    \n",
    "                    box_width = x3-x1\n",
    "                    box_height = y3-y1\n",
    "                            \n",
    "                    spatial_ratio = (box_width * box_height) / (img_width * img_height)\n",
    "                    \n",
    "                    if spatial_ratio < 1 / 14:\n",
    "                        boxes -= 1\n",
    "                        continue\n",
    "                         \n",
    "            if boxes == 0:\n",
    "                no_list.append(path)\n",
    "            else:\n",
    "                any_box = False\n",
    "                yes_list.append(path)\n",
    "        else:\n",
    "            no_list.append(path)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count\n",
    "len(no_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Detect no box {:.2%}'.format(len(no_list) / dataset.count))\n",
    "print('Detect any box {:.2%}'.format(len(yes_list) / dataset.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "random.shuffle(no_list)\n",
    "\n",
    "for no_detect_img in no_list[:100]:\n",
    "    img = PIL.Image.open(no_detect_img)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(yes_list)\n",
    "\n",
    "for yes_detect_img in yes_list[:100]:\n",
    "    img = PIL.Image.open(yes_detect_img)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for no_detect_img in no_list:\n",
    "    des = 'no_detect/'+ no_detect_img.split('/')[-1]\n",
    "    shutil.move(no_detect_img, des)\n",
    "\n",
    "for yes_detect_img in yes_list:\n",
    "    des = 'yes_detect/'+ yes_detect_img.split('/')[-1]\n",
    "    shutil.move(yes_detect_img, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no_detect_img in no_list:\n",
    "    des = 'no_detect_on/'+ no_detect_img.split('/')[-1]\n",
    "    shutil.move(no_detect_img, des)\n",
    "\n",
    "for yes_detect_img in yes_list:\n",
    "    des = 'yes_detect_on/'+ yes_detect_img.split('/')[-1]\n",
    "    shutil.move(yes_detect_img, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "source = 'yes_detect' # Screening 된 이미지들\n",
    "out = 'output/'\n",
    "\n",
    "if os.path.exists(out):\n",
    "    shutil.rmtree(out)  # delete output folder\n",
    "os.makedirs(out)  # make new output folder\n",
    "\n",
    "# Set Dataloader\n",
    "vid_path, vid_writer = None, None\n",
    "save_txt = False\n",
    "save_img = True\n",
    "dataset = LoadImages(source, img_size=img_size, half=half)\n",
    "\n",
    "# Get classes and colors\n",
    "classes = load_classes(parse_data_cfg(data)['names'])\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(classes))]\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    t = time.time()\n",
    "\n",
    "    # Get detections\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred, _ = model(img)\n",
    "\n",
    "    for i, det in enumerate(non_max_suppression(pred, conf_thres, nms_thres)):  # detections per image\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, s, im0 = path[i], '%g: ' % i, im0s[i]\n",
    "        else:\n",
    "            p, s, im0 = path, '', im0s\n",
    "\n",
    "        save_path = str(Path(out) / Path(p).name)\n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    \n",
    "            # Print results\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # detections per class\n",
    "                s += '%g %ss, ' % (n, classes[int(c)])  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, _, cls in det:\n",
    "                \n",
    "                if cls in (4,6,7,8): # 4 bag, 6 Headwear, 7 Acc, 8 Innerwear\n",
    "                    continue\n",
    "                    \n",
    "                if save_txt:  # Write to file\n",
    "                    with open(save_path + '.txt', 'a') as file:\n",
    "                        file.write(('%g ' * 6 + '\\n') % (*xyxy, cls, conf))\n",
    "                        \n",
    "                if save_img or view_img:  # Add bbox to image\n",
    "                    label = '%s %.2f' % (classes[int(cls)], conf)\n",
    "                    plot_one_box(xyxy, im0, label=label, color=colors[int(cls)])\n",
    "\n",
    "        print('%sDone. (%.3fs)' % (s, time.time() - t))\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'images':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:\n",
    "                if vid_path != save_path:  # new video\n",
    "                    vid_path = save_path\n",
    "                    if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                        vid_writer.release()  # release previous video writer\n",
    "\n",
    "                    fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                    w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                    h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*opt.fourcc), fps, (w, h))\n",
    "                vid_writer.write(im0)\n",
    "\n",
    "if save_txt or save_img:\n",
    "    print('Results saved to %s' % os.getcwd() + os.sep + out)\n",
    "    if platform == 'darwin':  # MacOS\n",
    "        os.system('open ' + out + ' ' + save_path)\n",
    "\n",
    "print('Done. (%.3fs)' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sypark",
   "language": "python",
   "name": "sypark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
