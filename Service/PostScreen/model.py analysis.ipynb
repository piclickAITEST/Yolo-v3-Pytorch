{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.google_utils import *\n",
    "from utils.parse_config import *\n",
    "from utils.utils import *\n",
    "\n",
    "ONNX_EXPORT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cfg/fashion.cfg'\n",
    "img_size=(416, 416)\n",
    "arc='default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse_model_cfg(path) in utils.parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parses the yolo-v3 layer configuration file and returns module definitions\n",
    "file = open(path, 'r')\n",
    "lines = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[net]',\n",
       " '# Testing',\n",
       " '#batch=1',\n",
       " '#subdivisions=1',\n",
       " '# Training',\n",
       " 'batch=64',\n",
       " 'subdivisions=4',\n",
       " 'width=416',\n",
       " 'height=416',\n",
       " 'channels=3',\n",
       " 'momentum=0.9',\n",
       " 'decay=5e-5',\n",
       " 'angle=0',\n",
       " 'saturation = 1.5',\n",
       " 'exposure = 1.5',\n",
       " 'hue=.1',\n",
       " '',\n",
       " 'learning_rate=1e-4',\n",
       " 'burn_in=1000',\n",
       " 'max_batches = 500200',\n",
       " 'policy=steps',\n",
       " 'steps=400000,450000',\n",
       " 'scales=.1,.1',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '# Downsample',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '# Downsample',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '# Downsample',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '# Downsample',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '# Downsample',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '######################',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[yolo]',\n",
       " 'mask = 6,7,8',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '',\n",
       " '',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '',\n",
       " '[route]',\n",
       " 'layers = -1, 61',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[yolo]',\n",
       " 'mask = 3,4,5',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '',\n",
       " '[route]',\n",
       " 'layers = -1, 36',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '',\n",
       " '',\n",
       " '[yolo]',\n",
       " 'mask = 0,1,2',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[net]',\n",
       " 'batch=64',\n",
       " 'subdivisions=4',\n",
       " 'width=416',\n",
       " 'height=416',\n",
       " 'channels=3',\n",
       " 'momentum=0.9',\n",
       " 'decay=5e-5',\n",
       " 'angle=0',\n",
       " 'saturation = 1.5',\n",
       " 'exposure = 1.5',\n",
       " 'hue=.1',\n",
       " 'learning_rate=1e-4',\n",
       " 'burn_in=1000',\n",
       " 'max_batches = 500200',\n",
       " 'policy=steps',\n",
       " 'steps=400000,450000',\n",
       " 'scales=.1,.1',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 6,7,8',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '[route]',\n",
       " 'layers = -1, 61',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 3,4,5',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '[route]',\n",
       " 'layers = -1, 36',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 0,1,2',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [x for x in lines if x and not x.startswith('#')] # 주석처리가 안된 부분의 line 가져옴\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[net]',\n",
       " 'batch=64',\n",
       " 'subdivisions=4',\n",
       " 'width=416',\n",
       " 'height=416',\n",
       " 'channels=3',\n",
       " 'momentum=0.9',\n",
       " 'decay=5e-5',\n",
       " 'angle=0',\n",
       " 'saturation = 1.5',\n",
       " 'exposure = 1.5',\n",
       " 'hue=.1',\n",
       " 'learning_rate=1e-4',\n",
       " 'burn_in=1000',\n",
       " 'max_batches = 500200',\n",
       " 'policy=steps',\n",
       " 'steps=400000,450000',\n",
       " 'scales=.1,.1',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=32',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=64',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=2',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=1024',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[shortcut]',\n",
       " 'from=-3',\n",
       " 'activation=linear',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=512',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=1024',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 6,7,8',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '[route]',\n",
       " 'layers = -1, 61',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=256',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=512',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 3,4,5',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1',\n",
       " '[route]',\n",
       " 'layers = -4',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[upsample]',\n",
       " 'stride=2',\n",
       " '[route]',\n",
       " 'layers = -1, 36',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'filters=128',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'batch_normalize=1',\n",
       " 'size=3',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=256',\n",
       " 'activation=leaky',\n",
       " '[convolutional]',\n",
       " 'size=1',\n",
       " 'stride=1',\n",
       " 'pad=1',\n",
       " 'filters=42',\n",
       " 'activation=linear',\n",
       " '[yolo]',\n",
       " 'mask = 0,1,2',\n",
       " 'anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       " 'classes=9',\n",
       " 'num=9',\n",
       " 'jitter=.3',\n",
       " 'ignore_thresh = .7',\n",
       " 'truth_thresh = 1',\n",
       " 'random=1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces , 공백 제거\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdefs = []  # module definitions\n",
    "for line in lines:\n",
    "    if line.startswith('['):  # This marks the start of a new block\n",
    "        mdefs.append({})\n",
    "        mdefs[-1]['type'] = line[1:-1].rstrip()\n",
    "        if mdefs[-1]['type'] == 'convolutional':\n",
    "            mdefs[-1]['batch_normalize'] = 0  # pre-populate with zeros (may be overwritten later)\n",
    "    else:\n",
    "        key, val = line.split(\"=\")\n",
    "        key = key.rstrip()\n",
    "\n",
    "        if 'anchors' in key:\n",
    "            mdefs[-1][key] = np.array([float(x) for x in val.split(',')]).reshape((-1, 2))  # np anchors\n",
    "        else:\n",
    "            mdefs[-1][key] = val.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'net',\n",
       "  'batch': '64',\n",
       "  'subdivisions': '4',\n",
       "  'width': '416',\n",
       "  'height': '416',\n",
       "  'channels': '3',\n",
       "  'momentum': '0.9',\n",
       "  'decay': '5e-5',\n",
       "  'angle': '0',\n",
       "  'saturation': '1.5',\n",
       "  'exposure': '1.5',\n",
       "  'hue': '.1',\n",
       "  'learning_rate': '1e-4',\n",
       "  'burn_in': '1000',\n",
       "  'max_batches': '500200',\n",
       "  'policy': 'steps',\n",
       "  'steps': '400000,450000',\n",
       "  'scales': '.1,.1'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': '-3', 'activation': 'linear'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '1024',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '42',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '6,7,8',\n",
       "  'anchors': array([[         10,          13],\n",
       "         [         16,          30],\n",
       "         [         33,          23],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': '9',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'},\n",
       " {'type': 'route', 'layers': '-4'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': '2'},\n",
       " {'type': 'route', 'layers': '-1, 61'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '512',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '42',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '3,4,5',\n",
       "  'anchors': array([[         10,          13],\n",
       "         [         16,          30],\n",
       "         [         33,          23],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': '9',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'},\n",
       " {'type': 'route', 'layers': '-4'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': '2'},\n",
       " {'type': 'route', 'layers': '-1, 36'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '256',\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'pad': '1',\n",
       "  'filters': '42',\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': '0,1,2',\n",
       "  'anchors': array([[         10,          13],\n",
       "         [         16,          30],\n",
       "         [         33,          23],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': '9',\n",
       "  'num': '9',\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': '1',\n",
       "  'random': '1'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return mdefs ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    def __init__(self, anchors, nc, img_size, yolo_index, arc):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.na = len(anchors)  # number of anchors (3)\n",
    "        self.nc = nc  # number of classes (80)\n",
    "        self.nx = 0  # initialize number of x gridpoints\n",
    "        self.ny = 0  # initialize number of y gridpoints\n",
    "        self.arc = arc\n",
    "\n",
    "    def forward(self, p, img_size, var=None):\n",
    "        bs, ny, nx = p.shape[0], p.shape[-2], p.shape[-1]\n",
    "        if (self.nx, self.ny) != (nx, ny):\n",
    "            create_grids(self, img_size, (nx, ny), p.device, p.dtype)\n",
    "\n",
    "        # p.view(bs, 255, 13, 13) -- > (bs, 3, 13, 13, 85)  # (bs, anchors, grid, grid, classes + xywh)\n",
    "        p = p.view(bs, self.na, self.nc + 5, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n",
    "        # Contiguous() -> Non-contiguous tensor to conti\n",
    "        if self.training:\n",
    "            return p\n",
    "\n",
    "        else:  # inference\n",
    "            # s = 1.5  # scale_xy  (pxy = pxy * s - (s - 1) / 2)\n",
    "            io = p.clone()  # inference output\n",
    "            io[..., 0:2] = torch.sigmoid(io[..., 0:2]) + self.grid_xy  # xy\n",
    "            io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh  # wh yolo method\n",
    "            # io[..., 2:4] = ((torch.sigmoid(io[..., 2:4]) * 2) ** 3) * self.anchor_wh  # wh power method\n",
    "            io[..., :4] *= self.stride\n",
    "\n",
    "            if 'default' in self.arc:  # seperate obj and cls\n",
    "                torch.sigmoid_(io[..., 4:])\n",
    "            elif 'BCE' in self.arc:  # unified BCE (80 classes)\n",
    "                torch.sigmoid_(io[..., 5:])\n",
    "                io[..., 4] = 1\n",
    "            elif 'CE' in self.arc:  # unified CE (1 background + 80 classes)\n",
    "                io[..., 4:] = F.softmax(io[..., 4:], dim=4)\n",
    "                io[..., 4] = 1\n",
    "\n",
    "            if self.nc == 1:\n",
    "                io[..., 5] = 1  # single-class model https://github.com/ultralytics/yolov3/issues/235\n",
    "\n",
    "            # reshape from [1, 3, 13, 13, 85] to [1, 507, 85]\n",
    "            return io.view(bs, -1, 5 + self.nc), p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_grids(self, img_size=416, ng=(13, 13), device='cpu', type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(self, img_size=416, ng=(13, 13), device='cpu', type=torch.float32):\n",
    "    nx, ny = ng  # x and y grid size\n",
    "    self.img_size = max(img_size)\n",
    "    self.stride = self.img_size / max(ng)\n",
    "\n",
    "    # build xy offsets\n",
    "    yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "    self.grid_xy = torch.stack((xv, yv), 2).to(device).type(type).view((1, 1, ny, nx, 2))\n",
    "\n",
    "    # build wh gains\n",
    "    self.anchor_vec = self.anchors.to(device) / self.stride # Stride에 대한 비율값\n",
    "    self.anchor_wh = self.anchor_vec.view(1, self.na, 1, 1, 2).to(device).type(type)\n",
    "    self.ng = torch.Tensor(ng).to(device)\n",
    "    self.nx = nx\n",
    "    self.ny = ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = (13,13)\n",
    "nx, ny = ng\n",
    "img_size = 416\n",
    "stride = img_size / max(ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)]) # (13,13) grid 좌표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "         [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "         [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "         [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "         [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "         [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "         [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\n",
       "         [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\n",
       "         [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\n",
       "         [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "         [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "         [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yv, xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xy = torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)) # 2 : x,y grid offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors= np.array([[         10,          13], # width, height \n",
    "         [         16,          30],\n",
    "         [         33,          23],\n",
    "         [         30,          61],\n",
    "         [         62,          45],\n",
    "         [         59,         119],\n",
    "         [        116,          90],\n",
    "         [        156,         198],\n",
    "         [        373,         326]])\n",
    "\n",
    "anchors = anchors[[0,1,2]]\n",
    "anchors = torch.Tensor(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 13.],\n",
       "        [16., 30.],\n",
       "        [33., 23.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = len(anchors) # number of anchors\n",
    "\n",
    "anchor_vec = anchors / stride\n",
    "anchor_wh = anchor_vec.view(1, na, 1, 1, 2)\n",
    "ng = torch.Tensor(ng)\n",
    "nx = nx\n",
    "ny = ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_modules(module_defs, img_size, arc) in models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_defs = mdefs ## parse_model_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = module_defs.pop(0)\n",
    "output_filters = [int(hyperparams['channels'])]\n",
    "module_list = nn.ModuleList()\n",
    "routs = []  # list of layers which rout to deeper layes\n",
    "yolo_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'net',\n",
       " 'batch': '64',\n",
       " 'subdivisions': '4',\n",
       " 'width': '416',\n",
       " 'height': '416',\n",
       " 'channels': '3',\n",
       " 'momentum': '0.9',\n",
       " 'decay': '5e-5',\n",
       " 'angle': '0',\n",
       " 'saturation': '1.5',\n",
       " 'exposure': '1.5',\n",
       " 'hue': '.1',\n",
       " 'learning_rate': '1e-4',\n",
       " 'burn_in': '1000',\n",
       " 'max_batches': '500200',\n",
       " 'policy': 'steps',\n",
       " 'steps': '400000,450000',\n",
       " 'scales': '.1,.1'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mdef in enumerate(module_defs):\n",
    "    modules = nn.Sequential() # 순차적으로 실행하도록 담는 Container 개념\n",
    "\n",
    "    if mdef['type'] == 'convolutional':\n",
    "        bn = int(mdef['batch_normalize'])\n",
    "        filters = int(mdef['filters'])\n",
    "        kernel_size = int(mdef['size'])\n",
    "        pad = (kernel_size - 1) // 2 if int(mdef['pad']) else 0\n",
    "        modules.add_module('Conv2d', nn.Conv2d(in_channels=output_filters[-1],\n",
    "                                               out_channels=filters,\n",
    "                                               kernel_size=kernel_size,\n",
    "                                               stride=int(mdef['stride']),\n",
    "                                               padding=pad,\n",
    "                                               bias=not bn))\n",
    "        if bn:\n",
    "            modules.add_module('BatchNorm2d', nn.BatchNorm2d(filters, momentum=0.1))\n",
    "        if mdef['activation'] == 'leaky':  # TODO: activation study https://github.com/ultralytics/yolov3/issues/441\n",
    "            modules.add_module('activation', nn.LeakyReLU(0.1, inplace=True))\n",
    "            # modules.add_module('activation', nn.PReLU(num_parameters=1, init=0.10))\n",
    "            # modules.add_module('activation', Swish())\n",
    "\n",
    "    elif mdef['type'] == 'upsample':\n",
    "        modules = nn.Upsample(scale_factor=int(mdef['stride']), mode='nearest')\n",
    "\n",
    "    elif mdef['type'] == 'route':  # nn.Sequential() placeholder for 'route' layer\n",
    "        layers = [int(x) for x in mdef['layers'].split(',')]\n",
    "        filters = sum([output_filters[i + 1 if i > 0 else i] for i in layers])\n",
    "        routs.extend([l if l > 0 else l + i for l in layers])\n",
    "        # if mdef[i+1]['type'] == 'reorg3d':\n",
    "        #     modules = nn.Upsample(scale_factor=1/float(mdef[i+1]['stride']), mode='nearest')  # reorg3d\n",
    "\n",
    "    elif mdef['type'] == 'shortcut':  # nn.Sequential() placeholder for 'shortcut' layer\n",
    "        filters = output_filters[int(mdef['from'])]\n",
    "        layer = int(mdef['from'])\n",
    "        routs.extend([i + layer if layer < 0 else layer])\n",
    "\n",
    "    elif mdef['type'] == 'yolo':\n",
    "        yolo_index += 1\n",
    "        mask = [int(x) for x in mdef['mask'].split(',')]  # anchor mask\n",
    "        modules = YOLOLayer(anchors=mdef['anchors'][mask],  # anchor list\n",
    "                            nc=int(mdef['classes']),  # number of classes\n",
    "                            img_size=img_size,  # (416, 416)\n",
    "                            yolo_index=yolo_index,  # 0, 1 or 2\n",
    "                            arc=arc)  # yolo architecture\n",
    "        \n",
    "        # Initialize preceding Conv2d() bias (https://arxiv.org/pdf/1708.02002.pdf section 3.3)\n",
    "        try:\n",
    "            if arc == 'defaultpw' or arc == 'Fdefaultpw':  # default with positive weights\n",
    "                b = [-4, -3.6]  # obj, cls\n",
    "            elif arc == 'default':  # default no pw (40 cls, 80 obj)\n",
    "                b = [-5.5, -4.0]\n",
    "            elif arc == 'uBCE':  # unified BCE (80 classes)\n",
    "                b = [0, -8.5]\n",
    "            elif arc == 'uCE':  # unified CE (1 background + 80 classes)\n",
    "                b = [10, -0.1]\n",
    "            elif arc == 'Fdefault':  # Focal default no pw (28 cls, 21 obj, no pw)\n",
    "                b = [-2.1, -1.8]\n",
    "            elif arc == 'uFBCE' or arc == 'uFBCEpw':  # unified FocalBCE (5120 obj, 80 classes)\n",
    "                b = [0, -6.5]\n",
    "            elif arc == 'uFCE':  # unified FocalCE (64 cls, 1 background + 80 classes)\n",
    "                b = [7.7, -1.1]\n",
    "\n",
    "            bias = module_list[-1][0].bias.view(len(mask), -1)  # 255 to 3x85\n",
    "            bias[:, 4] += b[0] - bias[:, 4].mean()  # obj\n",
    "            bias[:, 5:] += b[1] - bias[:, 5:].mean()  # cls\n",
    "            # bias = torch.load('weights/yolov3-spp.bias.pt')[yolo_index]  # list of tensors [3x85, 3x85, 3x85]\n",
    "            module_list[-1][0].bias = torch.nn.Parameter(bias.view(-1))\n",
    "            # utils.print_model_biases(model)\n",
    "        except:\n",
    "            print('WARNING: smart bias initialization failure.')\n",
    "\n",
    "    else:\n",
    "        print('Warning: Unrecognized Layer Type: ' + mdef['type'])\n",
    "\n",
    "    # Register module list and number of output filters\n",
    "    module_list.append(modules)\n",
    "    output_filters.append(filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Sequential(\n",
       "    (Conv2d): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (4): Sequential()\n",
       "  (5): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (8): Sequential()\n",
       "  (9): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (10): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (11): Sequential()\n",
       "  (12): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (13): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (14): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (15): Sequential()\n",
       "  (16): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (17): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (18): Sequential()\n",
       "  (19): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (20): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (21): Sequential()\n",
       "  (22): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (23): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (24): Sequential()\n",
       "  (25): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (26): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (27): Sequential()\n",
       "  (28): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (29): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (30): Sequential()\n",
       "  (31): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (32): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (33): Sequential()\n",
       "  (34): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (35): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (36): Sequential()\n",
       "  (37): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (38): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (39): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (40): Sequential()\n",
       "  (41): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (42): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (43): Sequential()\n",
       "  (44): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (45): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (46): Sequential()\n",
       "  (47): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (48): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (49): Sequential()\n",
       "  (50): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (51): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (52): Sequential()\n",
       "  (53): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (54): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (55): Sequential()\n",
       "  (56): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (57): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (58): Sequential()\n",
       "  (59): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (60): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (61): Sequential()\n",
       "  (62): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (63): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (64): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (65): Sequential()\n",
       "  (66): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (67): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (68): Sequential()\n",
       "  (69): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (70): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (71): Sequential()\n",
       "  (72): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (73): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (74): Sequential()\n",
       "  (75): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (76): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (77): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (78): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (79): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (80): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (81): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (82): Sequential(\n",
       "    (Conv2d): Conv2d(42, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (83): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (84): Sequential(\n",
       "    (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (85): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (86): Sequential()\n",
       "  (87): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (88): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (89): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (90): Sequential()\n",
       "  (91): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (92): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (93): Sequential()\n",
       "  (94): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (95): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (96): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (97): Sequential()\n",
       "  (98): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (99): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (100): Sequential()\n",
       "  (101): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (102): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (103): Sequential()\n",
       "  (104): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (105): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (106): Sequential()\n",
       "  (107): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (108): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (109): Sequential()\n",
       "  (110): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (111): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (112): Sequential()\n",
       "  (113): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (114): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (115): Sequential()\n",
       "  (116): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (117): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (118): Sequential()\n",
       "  (119): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (120): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (121): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (122): Sequential()\n",
       "  (123): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (124): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (125): Sequential()\n",
       "  (126): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (127): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (128): Sequential()\n",
       "  (129): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (130): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (131): Sequential()\n",
       "  (132): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (133): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (134): Sequential()\n",
       "  (135): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (136): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (137): Sequential()\n",
       "  (138): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (139): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (140): Sequential()\n",
       "  (141): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (142): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (143): Sequential()\n",
       "  (144): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (145): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (146): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (147): Sequential()\n",
       "  (148): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (149): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (150): Sequential()\n",
       "  (151): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (152): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (153): Sequential()\n",
       "  (154): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (155): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (156): Sequential()\n",
       "  (157): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (158): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (159): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (160): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (161): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (162): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (163): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (164): YOLOLayer()\n",
       "  (165): Sequential()\n",
       "  (166): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (167): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (168): Sequential()\n",
       "  (169): Sequential(\n",
       "    (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (170): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (171): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (172): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (173): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (174): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (175): Sequential(\n",
       "    (Conv2d): Conv2d(512, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (176): YOLOLayer()\n",
       "  (177): Sequential()\n",
       "  (178): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (179): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (180): Sequential()\n",
       "  (181): Sequential(\n",
       "    (Conv2d): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (182): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (183): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (184): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (185): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (186): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (187): Sequential(\n",
       "    (Conv2d): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (188): YOLOLayer()\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return module_list, routs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_yolo_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo_layers(model):\n",
    "    return [i for i, x in enumerate(model.module_defs) if x['type'] == 'yolo']  # [82, 94, 106] for yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    # YOLOv3 object detection model\n",
    "\n",
    "    def __init__(self, cfg, img_size=(416, 416), arc='default'):\n",
    "        super(Darknet, self).__init__()\n",
    "\n",
    "        self.module_defs = parse_model_cfg(cfg)\n",
    "        self.module_list, self.routs = create_modules(self.module_defs, img_size, arc)\n",
    "        self.yolo_layers = get_yolo_layers(self)\n",
    "\n",
    "        # Darknet Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n",
    "        self.version = np.array([0, 2, 5], dtype=np.int32)  # (int32) version info: major, minor, revision\n",
    "        self.seen = np.array([0], dtype=np.int64)  # (int64) number of images seen during training\n",
    "\n",
    "    def forward(self, x, var=None):\n",
    "        img_size = x.shape[-2:]\n",
    "        layer_outputs = []\n",
    "        output = []\n",
    "        # forward function\n",
    "        for i, (mdef, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "            mtype = mdef['type']\n",
    "            if mtype in ['convolutional', 'upsample', 'maxpool']:\n",
    "                x = module(x)\n",
    "            elif mtype == 'route':\n",
    "                layers = [int(x) for x in mdef['layers'].split(',')]\n",
    "                if len(layers) == 1:\n",
    "                    x = layer_outputs[layers[0]]\n",
    "                else:\n",
    "                    try:\n",
    "                        x = torch.cat([layer_outputs[i] for i in layers], 1)\n",
    "                    except:  # apply stride 2 for darknet reorg layer\n",
    "                        layer_outputs[layers[1]] = F.interpolate(layer_outputs[layers[1]], scale_factor=[0.5, 0.5])\n",
    "                        x = torch.cat([layer_outputs[i] for i in layers], 1)\n",
    "                    # print(''), [print(layer_outputs[i].shape) for i in layers], print(x.shape)\n",
    "            elif mtype == 'shortcut':\n",
    "                x = x + layer_outputs[int(mdef['from'])]\n",
    "            elif mtype == 'yolo':\n",
    "                x = module(x, img_size)\n",
    "                output.append(x)\n",
    "            layer_outputs.append(x if i in self.routs else [])\n",
    "\n",
    "        if self.training:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            io, p = list(zip(*output))  # inference output, training output\n",
    "            return torch.cat(io, 1), p\n",
    "\n",
    "    # for inference\n",
    "    def fuse(self):\n",
    "        # Fuse Conv2d + BatchNorm2d layers throughout model\n",
    "        fused_list = nn.ModuleList()\n",
    "        for a in list(self.children())[0]:\n",
    "            if isinstance(a, nn.Sequential):\n",
    "                for i, b in enumerate(a):\n",
    "                    if isinstance(b, nn.modules.batchnorm.BatchNorm2d):\n",
    "                        # fuse this bn layer with the previous conv2d layer\n",
    "                        conv = a[i - 1]\n",
    "                        fused = torch_utils.fuse_conv_and_bn(conv, b)\n",
    "                        a = nn.Sequential(fused, *list(a.children())[i + 1:])\n",
    "                        break\n",
    "            fused_list.append(a)\n",
    "        self.module_list = fused_list\n",
    "        # model_info(self)  # yolov3-spp reduced from 225 to 152 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(path, arc=arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (Conv2d): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): Sequential()\n",
       "    (5): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): Sequential()\n",
       "    (9): Sequential(\n",
       "      (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): Sequential()\n",
       "    (12): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): Sequential()\n",
       "    (16): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): Sequential()\n",
       "    (19): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (21): Sequential()\n",
       "    (22): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): Sequential()\n",
       "    (25): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Sequential()\n",
       "    (28): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): Sequential()\n",
       "    (31): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (33): Sequential()\n",
       "    (34): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Sequential()\n",
       "    (37): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): Sequential()\n",
       "    (41): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): Sequential()\n",
       "    (44): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): Sequential()\n",
       "    (47): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): Sequential()\n",
       "    (50): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (52): Sequential()\n",
       "    (53): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): Sequential()\n",
       "    (56): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): Sequential()\n",
       "    (59): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): Sequential()\n",
       "    (62): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (65): Sequential()\n",
       "    (66): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): Sequential()\n",
       "    (69): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): Sequential()\n",
       "    (72): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): Sequential()\n",
       "    (75): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (79): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (80): Sequential(\n",
       "      (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (81): Sequential(\n",
       "      (Conv2d): Conv2d(1024, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (82): YOLOLayer()\n",
       "    (83): Sequential()\n",
       "    (84): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (86): Sequential()\n",
       "    (87): Sequential(\n",
       "      (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (89): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (90): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (91): Sequential(\n",
       "      (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (92): Sequential(\n",
       "      (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (93): Sequential(\n",
       "      (Conv2d): Conv2d(512, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (94): YOLOLayer()\n",
       "    (95): Sequential()\n",
       "    (96): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (97): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (98): Sequential()\n",
       "    (99): Sequential(\n",
       "      (Conv2d): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (101): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (102): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (103): Sequential(\n",
       "      (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (104): Sequential(\n",
       "      (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (105): Sequential(\n",
       "      (Conv2d): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (106): YOLOLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7fc1b018b4c0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwhyun",
   "language": "python",
   "name": "jwhyun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
